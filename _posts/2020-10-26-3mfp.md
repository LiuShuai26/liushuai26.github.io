---
layout: post
title:  "Model-Free Prediction"
date:   2020-10-26 01:20:00 +0800
categories: rl




---

> Reinforcement Learning basic concepts

Estimate the value function of an unknown MDP.

* Monte Carlo Prediction
* Temporal-Difference (TD) Prediction

---

Monte Carlo Methods

Monte Carlo methods require only experience---sample sequences of states, actions, and rewards from actual or simulated interaction with an environment.

* MC methods learn directly from episodes of experience

* MC is model-free: no knowledge of MDP transitions / rewards

* MC learns from complete episodes: no bootstrapping

* MC uses the simplest possible idea: value = mean return

Here we deﬁne Monte Carlo methods only for episodic tasks. That is, we assume experience is divided into episodes, and that all episodes eventually terminate no matter what actions are selected.

TD methods

- TD methods learn directly from episodes of experience

- TD is model-free: no knowledge of MDP transitions / rewards

- TD learns from incomplete episodes, by bootstrapping

- TD updates a guess towards a guess

---

### Monte Carlo Prediction

An obvious way to estimate it from experience, then, is simply to average the returns observed after visits to that state. As more returns are observed, the average should converge to the expected value. 

Goal: learn $v_\pi$ from episodes of experience under policy $\pi$

return is the total discounted reward:

$G_t = R_{t+1} + \gamma R_{t+2} + ... + \gamma^{T-1}R_T$



Each average is itself an unbiased estimate, and the standard deviation of its error falls as $1/\sqrt n$, where $n$ is the number of returns averaged. Whereas the DP diagram shows all possible transitions, the Monte Carlo diagram shows only those sampled on the one episode. Monte Carlo methods do not bootstrap. One can generate many sample episodes starting from the states of interest, averaging returns from only these states, ignoring all others.

### TD Prediction

Firstly, recall the monte carlo method. Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V(S_t)$. A simple every-visit Monte Carlo method suitable for nonstationary environments is


$$
V(S_t) \leftarrow V(S_t) + \alpha[G_t-V(S_t)]
$$


Whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(St)$ (only then is $G_t$ known), TD methods need to wait only until the next time step. At time $t + 1$ they immediately form a target and make a useful update using the observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$. The simplest TD method makes the update:


$$
V(S_t) \leftarrow V(S_t) + \alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_t)]
$$



The target for the TD update is **$R_{t+1}+\gamma V(S_{t+1})$**

Measuring the difference between **the estimated value of $S_t$** and **the better estimate $R_{t+1}+\gamma V(S_{t+1})$**. This quantity, called the $TD\ error$:


$$
\delta_t\ \dot =\ R_{t+1} + \gamma V(S_{t+1})-V(S_t)
$$


#### MC and TD

Goal: learn $v_\pi$ online from experience under policy $\pi$

Incremental every-visit Monte-Carlo

* update value $V(S_t)$ toward *actual* return $G_t$

  
  $$
  V(S_t) \leftarrow V(S_t) + \alpha (G_t-V(S_t))
  $$

Simplest temporal-difference learning algorithm: TD(0)

* Update value $V(S_t)$ toward *estimated* return $R_{t+1}+\gamma V(S_{t+1})$

  
  $$
  V(S_t) \leftarrow V(S_t) + \alpha(R_{t+1}+\gamma V(S_{t+1})- V(S_t))
  $$

#### Advantages and Disadvantages of MC vs. TD

TD can learn before knowing the final outcome

* TD can learn online after every step 
* MC must wait until end of episode before return is known 

TD can learn without the final outcome 

* TD can learn from incomplete sequences
* MC can only learn from complete sequences
* TD works in continuing (non-terminating) environments
* MC only works for episodic (terminating) environments

TD methods have usually been found to converge faster than constant-$\alpha$ MC methods on stochastic tasks

---

## Model-Free Control

Policy improvement.

- Monte-Carlo Control
- TD Control

Model-Free Policy Iteration Using Action-Value Function.

Greedy policy improvement over V(s) requires model of MDP


$$
\pi'(s) = \underset{a \in A} {\arg\max} \cal R^a_s+\cal P^a_{ss'}V(s')
$$

Policy improvement is done by making the policy greedy with respect to the current value function. In this case we have an action-value function, and therefore no model is needed to construct the greedy policy.

Greedy policy improvement over $Q(s, a)$ is model-free


$$
\pi'(s) = \underset {a \in A} {\arg \max} Q(s, a)
$$


problem about greedy action selection: never exploration.

#### $\epsilon$-Greedy Exploration

$\epsilon$-Greedy, Simplest idea for ensuring continual exploration. meaning that most of the time they choose an action that has maximal estimated action value, but with probability $\epsilon$ they instead select an action at random.

### Monte-Carlo Control



### TD Control

Temporal-difference (TD) learning has several advantages over Monte-Carlo (MC) 

* Lower variance 
* Online 
* Incomplete sequences 

Natural idea: use TD instead of MC in our control loop 

* Apply TD to Q(S,A) 
* Use $\epsilon$-greedy policy improvement 
* **Update every time-step**

### Sarsa: On-policy TD Control

$$
Q(S, A) \leftarrow Q(S, A) + \alpha (R+\gamma Q(S', A')-Q(S,A))
$$

This update is done after every transition from a nonterminal state St. If $S'$ is terminal, then $Q(S', A')$ is defined as zero.



n-step Sarsa

n = 1 (Sarsa)                 $q_t^{(1)} = R_{t+1} + \gamma Q(S_{t+1})$

n = 2                              $q_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 Q(S_{t+2})$

...

n = $\infty$ (MC)                   $q_t^{(\infty)} = R_{t+1} + \gamma R_{t+2} +\ ...\ + \gamma^{T-1}R_T$

n-step Q-return              $q_t^{(n)} = R_{t+1} + \gamma R_{t+2} +\ ...\ + \gamma^{n-1}R_{t+n}+\gamma^nQ(S_{t+n})$

n-step Sarsa updates Q(s,a) towards the n-step Q-return:


$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(q_t^{(n)}-Q(S_t, A_t))
$$


**Sarsa(λ) Algorithm: **



#### Off-Policy Learning

Evaluate target policy $\pi(a\|s)$ to compute $v_\pi(s)$ or $q_\pi(s, a)$

While following behavior policy $\mu(a\|s)$


$$
\{S_1,A_1,R_2,\ ...,S_T\}\sim \mu
$$


Re-use experience generated from old policies $\pi_1, \pi_2, \ ..., \pi_{t-1}$, Learn from observing humans or other agents.

Learn about optimal policy while following exploratory policy

Learn about multiple policies while following one policy

#### Q-Learning: Off-policy TD Control

consider oﬀ-policy learning of action-values $Q(s, a)$

Next action is chosen using behavior policy $A_{t+1} \sim \mu(\cdot\|S_t)$

But we consider alternative successor action $A' \sim \pi(\cdot\|S_t)$

And update $Q(S_t, A_t)$ towards value of alternative action


$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(R_{t+1}+\gamma Q(S_{t+1}, A')-Q(S_t, A_t))
$$

The target policy $\pi$ is greedy w.r.t $Q(s, a)$


$$
\pi(S_{t+1}) = \underset {a'} {\arg\max} Q(S_{t+1}, a')
$$


The behavior policy $\mu$ is e.g. $\epsilon$-greedy w.r.t $Q(s, a)$


$$
\begin{align}
&\ \ \  \ \  R_{t+1} + \gamma Q(S_{t+1}, A') \\
& = R_{t+1} + \gamma Q(S_{t+1}, \underset {a'} {\arg\max}Q(S_{t+1}, a')) \\
& = R_{t+1} + \underset {a'} \max \gamma Q(S_{t+1}, a')
\end{align}
$$


Q-Learning Control Algorithm


$$
Q(S, A) \leftarrow Q(S, A) + \alpha(R+\gamma \underset {a'} \max Q(S', a')-Q(S, A))
$$


Sarsa (State-Action-Reward-State-Action):
The update is based on the action actually taken by the agent.
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( R_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right)
$$
Sarsa uses the next action  a_{t+1}  chosen by the current policy, making it an on-policy algorithm.
Q-learning:
The update is based on the maximum possible action-value from the next state, regardless of the action actually taken.
$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( R_{t+1} + \gamma \max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \right)
$$
Q-learning uses the maximum possible reward from the next state, making it an off-policy algorithm.
