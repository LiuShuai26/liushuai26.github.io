---
layout: post
title:  "DQN Algorithms"
date:   2020-12-26 01:20:00 +0800
categories: rl




---

> DQN and it's improvement.

## DQN

Goal is maximizes future rewards.

Define the future discounted return at time t as $R_t = \sum^T_{t'=t}\gamma^{t'-t}r_{t'}$. T is the time-step at which the games terminates. (Assume the future rewards are discounted by a factor of $\gamma$ per time-step.)

Define the optimal action-value function $Q^*(s, a)$ as the maximum expected return. ($\cal E$ : Emulator)


$$
Q^*(s, a) = \Bbb E_{s' \sim \cal E}[r+\gamma \underset {a'} \max Q^*(s', a')|s, a]
$$


function approximator to estimate the action-value function $Q(s, a; \theta) \approx Q^*(s, a)$

non-linear function approximator: neural network $Q(s, a; \theta) \approx Q^*(s, a)$

Loss function $L_i(\theta_i)$


$$
L_i(\theta_i) = \Bbb E_{s,a \sim \rho(\cdot)}[(y_i - Q(s, a;\theta_i))^2]\\
y_i = \Bbb E_{s' \sim \cal E}[r + \gamma \underset {a'} \max Q(s', a'; \theta_{i-1})|s, a]
$$


$y_i$ is the target.


$$
\nabla_{\theta_i}L_i(\theta_i) = \Bbb E_{s,a \sim \rho(\cdot);s'\sim \cal E}[(r+\gamma \underset {a'} \max Q(s', a'; \theta_{i-1})-Q(s, a;\theta_i))\nabla_{\theta_i}Q(s, a;\theta_i)]
$$


greedy strategy $a = \underset a \max Q(s, a; \theta)$ with probability $1 - \varepsilon $ and selects a random action with probability $\varepsilon$

two mechanisms make it work:

**Fixed Q-targets**:

- Using a separate network with a fixed parameter (let’s call it w-) for estimating the TD target.
- At every $\tau$ step, we copy the parameters from our DQN network to update the target network.

**Experience replay**: making more efficient use of observed experience:

- Avoid forgetting previous experiences.
- Reduce correlations between experiences.

---

## Double Q-learning

Q-learning, it is known to sometimes learn unrealistically high action values because it includes a maximization step over estimated action values, which tends to prefer overestimated to underestimated values. 

Overoptimize due to estimation errors.

 

The idea of Double Q-learning is to reduce overestimations by decomposing the max operation in the target into action selection and action evaluation. Although not fully decoupled, the target network in the DQN architecture provides a natural candidate for the second value function, without having to introduce additional networks. **We therefore propose to evaluate the greedy policy according to the online network, but using the target network to estimate its value.** 

 

In reference to both Double Q-learning and DQN, we refer to the resulting algorithm as Double DQN.


$$
Y_t^{DQN} = R_{t+1}+\gamma \underset a \max Q(S_{t+1}, a; \theta_t^-)
$$


It's update is the same as for DQN, but replacing the target $Y_t^{DQN}$ with


$$
Y_t^{DDQN} = R_{t+1}+\gamma Q(S_{t+1}, \underset a {\arg\max}Q(S_{t+1}, a;\theta_t), \theta_t^-)
$$


Why Double Q-learning work?

Theorem 1 in the paper.

---

## Dueling DQN

The key insight behind our new architecture is that for many states, it is unnecessary to estimate the value of each action choice.

The proposed network architecture, which we name the dueling architecture, explicitly separates the representation of state values and (state-dependent) action advantages. The dueling architecture consists of two streams that represent the value and advantage functions, while sharing a common convolutional feature learning module. The two streams are combined via a special aggregating layer to produce an estimate of the state-action value function Q.

From the expressions for advantage $Q^\pi(s, a) = V^\pi(s) + A^\pi(s, a)$ and state-value $V^\pi(s) = \Bbb E_{a \sim \pi(s)}[Q^\pi(s, a)]$, it follows that $\Bbb E_{a \sim \pi(s)}[A^\pi(s, a)] = 0$. Moreover, for a deterministic policy, $a^* = \arg\max_{a'\in\cal A}Q(s, a')$, it follows that $Q(s, a^*)=V(s)$ and hence $A(s, a^*) = 0$.

Using the deﬁnition of advantage, we might be tempted to construct the aggregating module as follows: 


$$
Q(s, a;\theta, \alpha, \beta) = V(s;\theta,\beta)+A(s,a;\theta,\alpha) \tag 7
$$


$V(s;\theta,\beta)$ is a scalar and $A(s,a;\theta,\alpha)$ is a $\|\cal A\|$-dimensional vector.

Equation above is unidentifiable in the sense that given Q we cannot recover  V and A uniquely.

To address this issue of identifiability, we can force the advantage function estimator to have zero advantage at the chosen action.


$$
Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta)+ \left(A(s,a;\theta,\alpha)-\underset {a'\in|\cal A|} \max A(s,a';\theta,\alpha)\right) \tag 8
$$


Now, for $a^*=\arg\max_{a'\in\cal A}Q(s,a';\theta,\alpha,\beta) = \arg\max_{a'\in\cal A}A(s,a';\theta,\alpha)$, we obtain $Q(s, a^*;\theta,\alpha, \beta)=V(s;\theta,\beta)$. Hence, the stream $V(s;\theta,\beta)$ provides an estimate of the value function, while the other stream produces an estimate of the advantage function.

An alternative module replaces the max operator with an average:


$$
Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta)+ \left(A(s,a;\theta,\alpha)- \frac 1 {|\cal A|} \underset {a'} \sum A(s,a';\theta,\alpha)\right) \tag 9
$$


On the one hand this loses the original semantics of V and A because they are now off-target by a constant, but on the other hand it increases the stability of the optimization: with (9) the advantages only need to change as fast as the mean, instead of having to compensate any change to the optimal action’s advantage in (8). 

The advantage of the dueling architecture lies partly in its ability to learn the state-value function efﬁciently. With every update of the Q values in the dueling architecture, the value stream V is updated – this contrasts with the updates in a single-stream architecture where only the value for one of the actions is updated, the values for all other actions remain untouched. This more frequent updating of the value stream in our approach allocates more resources to V , and thus allows for better approximation of the state values, which in turn need to be accurate for temporal difference-based methods like Q-learning to work.

---

## Prioritized Experience Replay

The key idea is that an RL agent can learn more effectively from some transitions than from others. 

In particular, we propose to more frequently replay transitions with high expected learning progress, as measured by the magnitude of their temporal-difference (TD) error.

TD error $\delta$, which indicates how ‘surprising’ or unexpected the transition is: speciﬁcally, how far the value is from its next-step bootstrap estimate.

In particular, we propose to more frequently replay transitions with high expected learning progress, as measured by the magnitude of their temporal-difference (TD) error. This prioritization can lead to a loss of diversity, which we alleviate with stochastic prioritization, and introduce bias, which we correct with importance sampling. Our resulting algorithms are robust and scalable, which we demonstrate on the Atari 2600 benchmark suite, where we obtain faster learning and state-of-the-art performance.

we use absolute TD error because it appears to be a reasonable approximation to the “usefulness” of sample

a KL-based one in Rainbow DQN

We can not keep a tally of all the magnitude of TD errors updated. DeepMind proposes a far more computationally efficient alternative of only updating the $\delta$ terms for items that are actually sampled during the minibatch gradient updates. Since we have to compute $\delta$ anyway to get the loss, we might as well use those to change the priorities. 

Next, given the absolute TD terms, how do we get a probability distribution for sampling? DeepMind proposes two ways of getting priorities, denoted as $p_i$:

- indirect: A *rank* based method: $p_i = \frac 1 {rank(i)}$ where $rank(i)$ is the rank of transition $i$ when the replay memory is sorted according to $\|\delta\|$
- direct: A *proportional* variant: $p_i = \|\delta_i\|+\epsilon$, where $\epsilon$ is a small constant ensuring that the sample has some non-zero probability of being drawn.

PER initializes $p_i$ according to the maximum priority of any priority thus far, thus favoring those terms during sampling later.

From either of these, we can easily get a probability distribution: $P(i) = \frac {p_i^\alpha} {\sum_k p_k^\alpha}$ The exponent $\alpha$ determines how much prioritization is used, with $\alpha = 0$ corresponding to the uniform case.

Since the buffer size N can be quite large (e.g., one million), DeepMind uses special data structures to reduce the time complexity of certain operations. For the proportional-based variant, which is what OpenAI implements, a sum-tree data structure is used to make both updating and sampling O(logN) operations.

The estimation of the expected value with stochastic updates relies on those updates corresponding to the same distribution as its expectation. Prioritized replay introduces bias because it changes this distribution in an uncontrolled fashion, and therefore changes the solution that the estimates will converge to (even if the policy and state distribution are fixed). We can correct this bias by using importance-sampling (IS) weights.

How do we apply importance sampling? We use the following weights:


$$
w_i = (\frac 1 N \cdot \frac 1 {P(i)})^\beta
$$


The $\frac 1 N$ part is because of the current experience replay size.

I think the distribution DeepMind is talking about (“same distribution as its expectation”) above is the distribution of samples that are obtained when sampling uniformly at random from the replay buffer.

*PER over-samples those with high priority*, so the importance sampling correction should *down-weight* the impact of the sampled term, which it does by scaling the gradient term so that the gradient has “less impact” on the parameters.

the importance sampling in PER is to correct the over-sampling with respect to the uniform distribution.

See more in https://danieltakeshi.github.io/2019/07/14/per/

---

## Rainbow 

Rainbow = 

Double DQN + Prioritized experience replay + Dueling network architecture + Learning from multi-step bootstrap targets + Distributional Q-learning + Noisy DQN


