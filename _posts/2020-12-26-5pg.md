---
layout: post
title:  "Policy Gradient Method"
date:   2020-12-26 01:20:00 +0800
categories: rl




---

> Reinforcement Learning basic concepts

Value Based: A policy was generated directly from the value function e.g. using $\epsilon$-greedy

In this lecture we will directly parametrize the policy

Advantages: 

* Better convergence properties 
* Eﬀective in high-dimensional or continuous action spaces 
* Can learn stochastic policies 

Disadvantages: 

* Typically converge to a local rather than global optimum 
* Evaluating a policy is typically ineﬃcient and high variance

---

We consider the case of a stochastic, parameterized policy, $\pi_\theta$. Our goal is given policy $\pi_\theta(s,a)$ with parameters $\theta$, ﬁnd best $\theta$. **But how do we measure the quality of a policy $\pi_\theta$?** We use policy objective functions $J(\theta)$. We aim to maximize the expected return $J(\theta)$.

In episodic environments we can use the **start value**


$$
J_1(\theta) = V^{\pi_\theta}(s_1) = \Bbb E_{\pi_\theta}[v_1]
$$


In continuing environments we can use the **average value**


$$
J_{avV}(\theta) = \underset s \sum d^{\pi_\theta}(s)V^{\pi_\theta}(s)
$$


Or the **average reward per time-step**


$$
J_{avR}(\theta) = \underset s \sum d^{\pi_\theta}(s) \underset a \sum \pi_\theta(s,a)\cal R^a_s
$$


where $d^{\pi_\theta}(s)$ is **stationary distribution** of Markov chain for $\pi_\theta$

---

Now on, Policy based reinforcement learning is an **optimization** problem.

Then Find $\theta$ that maximizes $J(\theta)$ using gradient ascend, eg


$$
\theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\pi_\theta)|_{\theta_k}
$$


The gradient of policy performance, $\nabla_\theta J(\pi_\theta)$, is called the **policy gradient**, and algorithms that optimize the policy this way are called **policy gradient algorithms.**

---

Score Function describes how much better or worse it is than other actions on average (relative to the current policy) in state s.

Assume policy $\pi_\theta$ is differentiable whenever is non-zero and we know the gradient $\nabla_\theta\pi_\theta(s, a)$

**Likelihood ratios** exploit the following identity


$$
\nabla_\theta\pi_\theta(s, a) = \pi_\theta(s,a)\frac {\nabla_\theta\pi_\theta(s,a)}{\pi_\theta(s,a)}\\
=\pi_\theta(s, a)\nabla_\theta\log\pi_\theta(s, a)
$$


The **score function** is $\nabla_\theta\log\pi_\theta(s, a)$

in discrete action spaces, Softmax Policy, score function is 


$$
\nabla_\theta\log\pi_\theta(s, a) = \phi(s,a)-\Bbb E_{\pi_\theta}[\phi(s, \cdot)]
$$

---

### Policy Gradient Theorem

The policy gradient theorem generalizes the likelihood ratios approach to multi-step MDPs

$Q^\pi(s,a)$ here is long-term value, could be reward $R_t$.

>Theorem
>
>For any differentiable policy $\pi_\theta(s, a)$
>
>for any of the policy objective functions $J=J_1,J_{avR}, \frac 1 {1-\gamma}J_{avR}$, the policy gradient is 
>$$
>\nabla_\theta J(\theta) = \Bbb E_{\pi_\theta}[\nabla_\theta\log{\pi_\theta}(s,a)Q^{\pi_\theta}(s,a)]
>$$

---

### Implementing the Simplest Policy Gradient

(For full code click [here](https://github.com/openai/spinningup/blob/master/spinup/examples/pg_math/1_simple_pg.py).)

```python
# make core of policy network
obs_ph = tf.placeholder(shape=(None, obs_dim), dtype=tf.float32)
logits = mlp(obs_ph, sizes=hidden_sizes+[n_acts])

# make action selection op (outputs int actions, sampled from policy)
actions = tf.squeeze(tf.multinomial(logits=logits,num_samples=1), axis=1)
```

This block builds a feedforward neural network categorical policy.

```python
# make loss function whose gradient, for the right data, is policy gradient
weights_ph = tf.placeholder(shape=(None,), dtype=tf.float32)
act_ph = tf.placeholder(shape=(None,), dtype=tf.int32)
action_masks = tf.one_hot(act_ph, n_acts)
log_probs = tf.reduce_sum(action_masks * tf.nn.log_softmax(logits), axis=1)
loss = -tf.reduce_mean(weights_ph * log_probs)
```

In this block, we build a “loss” function for the policy gradient algorithm. When the right data is plugged in, the gradient of this loss is equal to the policy gradient. The right data means a set of (state, action, weight) tuples collected while acting with environment according to the current policy, where the **weight** for a state-action pair is the reward $R_t$.

---

### Reducing Variance using a Critic

Monte-Carlo policy gradient still has high variance.

We use a **critic** to estimate the action-value function


$$
Q_w(s, a) \approx Q^{\pi_\theta}(s, a)
$$


Actor-critic algorithms maintain two sets of parameters

**Critic** Updates action-value function parameters $w$

**Actor** Updates policy parameters $\theta$, in direction suggested by critic

Actor-critic algorithms follow an *approximate* policy gradient


$$
\nabla_\theta J(\theta) \approx \Bbb E_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)\ Q_w(s,a)] \\
\Delta_\theta = \alpha \nabla_\theta\log \pi_\theta(s,a)\ Q_w(s,a)
$$

---

### Bias in Actor-Critic Algorithms

we can avoid introducing any bias if we choose value function approximation carefully.

---

### Reducing Variance using a baseline

because EGLP lemma, We have $\underset {a_t\sim \pi_\theta} E [\nabla_\theta\log\pi_\theta(a_t\|s_t)b(s_t)] = 0$

This allows us to add or subtract any number of terms like this from our expression for the policy gradient, without changing it in expectation:


$$
\nabla_\theta J(\theta) = \Bbb E_{\pi_\theta}\left[\nabla_\theta\log{\pi_\theta}(s,a)\left(Q^{\pi_\theta}(s,a)-b(s)\right)\right]
$$


A good baseline is the state value function $b(s) = V^{\pi_\theta}(s)$

So we can rewrite the policy gradient using the advantage function $A^\pi(s_t, a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t)$

describes how much better or worse it is than other actions on average (relative to the current policy).

---

### Estimating the Advantage function

The advantage function can signiﬁcantly reduce variance of policy gradient 

So the critic should really estimate the advantage function 

For example, by estimating both $V^{\pi_\theta}(s)$ and $Q^{\pi_\theta}(s,a)$

Using two function approximators and two parameter vectors, 


$$
V_w(s) \approx V^{\pi_\theta}(s)\\
Q_w(s,a) \approx Q^{\pi_\theta}(s,a)\\
A(s,a) = Q_w(s,a) - V_w(s)
$$


And updating both value functions by e.g. TD learning

---

Reference:

Reinforcement Learning An Introduction

https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html

http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf