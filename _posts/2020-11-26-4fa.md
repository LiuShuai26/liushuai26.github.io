---
layout: post
title:  "Function Approximation"
date:   2020-11-24 01:20:00 +0800
categories: rl




---

> Reinforcement Learning basic concepts

Introduction

* Large-Scale RL
* Value Function Approximation
* Which Function Approximator

Incremental Methods

* Gradient Descent

* Value Function Approx. by SGD

* Feature Vectors, Linear Value Function Approximation

  Table lookup is a special case of linear value function approximation

* Incremental Prediction Algorithms

* MC, TD, TD(gamma), Control with Value Function Approximation

* (Linear) Action-Value Function Approximation

* Incremental Control Algorithms

* Should we Bootstrap. MC performance is really bad

* Algorithms Convergence

Batch Methods

Gradient descent is simple and appealing But it is not sample eﬃcient 

Batch methods seek to ﬁnd the best ﬁtting value function 

Given the agent’s experience (“training data”)                    off policy

* Least Squares Prediction
* SGD with Experience Replay
* Experience Replay in DQN
* Linear Least Squares Prediction
* Least Squares Policy Iteration
* Least Squares
  * Action-Value Function Approximation
  * Control
  * Q-Learning
  * Policy Iteration Algorithm

### the motivation for Function Approximation over Table Lookup

So far we have represented value function by a lookup table. Every state s has an entry $V(s)$ or every state-action pair s,a has an entry $Q(s, a)$.

Problem with large MDPs: There are too many states and/or actions to store in memory It is too slow to learn the value of each state individually.

**Function approximation** because it takes examples from a desired function (e.g., a value function) and attempts to generalize from them to construct an approximation of the entire function.

Solution for large MDPs: Estimate value function with **function approximation**

function approximation

$\hat{v}(s, w) \approx v_\pi(s) $ or $\hat{q}(s, a, w) \approx q_\pi(s, a)$

*the number of weights (the dimensionality of w) is much less than the number of states. Generalize from seen states to unseen states. Update parameter w using MC or TD learning.*

Which Function Approximator? We consider diﬀerentiable function approximators like Linear combinations of features and Neural network.

Furthermore, we require a training method that is suitable for non-stationary, non-iid data.

------

### Incremental Methods

#### Value Function Approx. By Stochastic Gradient Descent

Goal: ﬁnd parameter vector **w** minimizing mean-squared error between approximate value fn $\hat{v}(s, w)$ and true value fn $v_\pi(s)$


$$
J(w) = \Bbb E_\pi[(v_\pi(S)-\hat{v}(S, w))^2]
$$


Gradient descent ﬁnds a local minimum


$$
\begin{align}
\Delta w & = - \frac{1}{2}\alpha\nabla_wJ(w)\\
& =\alpha \Bbb E_\pi[(v_\pi(S)-\hat{v}(S,w))\nabla_\pi\hat{v}(S, w)]
\end{align}
$$


Stochastic gradient descent samples the gradient


$$
\Delta w = \alpha(v_\pi(S)-\hat{v}(S, w))\nabla_\pi\hat{v}(S, w)
$$


*Expected update is equal to full gradient update.*

#### Incremental Prediction Algorithms

All of the prediction methods covered here have been described as updates to an estimated value function that shift its value at particular states toward a “backed-up value,” or update target, for that state.

Have assumed true value function $v_\pi(s)$ given by supervisor But in RL there is no supervisor, only rewards In practice, we substitute a target for $v_\pi(s)$

For MC, the target is the return $G_t$


$$
\Delta w = \alpha (G_t - \hat{v}(S_t, w))\nabla_w\hat{v}(S_t, w)
$$


Monte-Carlo evaluation converges to a local optimum Even when using non-linear value function approximation

For TD(0), the target is the TD target $R_{t+1}+\gamma\hat{v}(S_{t+1}, w)$


$$
\Delta w = \alpha (R_{t+1}+\gamma\hat{v}(S_{t+1}, w) - \hat{v}(S_t, w))\nabla_w\hat{v}(S_t, w)
$$


Linear TD(0) converges (close) to global optimum

For TD($\gamma$), the target  is the $\gamma$-return $G_t^\gamma$


$$
\Delta w = \alpha (G_t^\gamma - \hat{v}(S_t, w))\nabla_w\hat{v}(S_t, w)
$$



TD does not follow the gradient of any objective function. This is why TD can diverge when oﬀ-policy or using non-linear function approximation Gradient TD follows true gradient of projected Bellman error.



*Reinforcement learning with function approximation involves a number of new issues that do not normally arise in conventional supervised learning, such as nonstationary, bootstrapping, and delayed targets.*



### Linear Methods

Linear methods approximate the state-value function by the inner product between w and x(s):


$$
\hat{v}(s, w) \dot= w^Tx(s) \dot=\overset d {\underset {i=1} \sum} w_ix_i(s)
$$

------

### Batch Methods

Gradient descent is simple and appealing But it is not sample eﬃcient.

Batch methods seek to ﬁnd the best ﬁtting value function, given the agent’s experience (“training data”) off policy

#### Least squares Prediction

Least squares algorithms ﬁnd parameter vector w minimizing sum-squared error between $\hat{v}(s_t, w)$ and target values $v^\pi_t$


$$
LS(w) = \overset T {\underset {t=1} \sum}(v^\pi_t-\hat{v}(s_t, w))^2 \\
= \Bbb E_D[(v^\pi-\hat{v}(s, w))^2]
$$


Given experience consisting of <state,value> pairs , $<s_1, v_1^\pi>$

Repeat:

1. Sample state, value from experience $<s, v^\pi>$

2. Apply stochastic gradient descent update

   
   $$
   \Delta w = \alpha (v^\pi-\hat{v}(s, w))\nabla_w\hat{v}(s, w)
   $$



Converges to least squares solution


$$
w^\pi = \underset w {\arg\min}\ LS(w)
$$


batching using experience replay: Store experience as dataset, randomize it, and repeatedly apply minibatch SGD.

Tricks to stabilize non-linear function approximators: Fixed Targets. The target is calculated based on frozen parameter values from a previous time step.

DQN uses experience replay and ﬁxed Q-targets.

------

