---
layout: post
title:  "Multi-armed Bandits: exploration and exploitation"
date:   2020-8-26 01:20:00 +0800
categories: rl




---

> Reinforcement Learning basic concepts

You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps. 

### The Multi-Armed Bandit

A multi-armed bandit is a tuple $\langle \cal A, R\rangle$

$\cal A$ is a known set of $m$ actions (or “arms”)

$\cal R^a(r) = \Bbb P[r\|a]$ is an unknown probability distribution over rewards

At each step $t$ the agent selects an action $a_t \in \cal A$

The environment generates a reward $r_t \sim \cal R^{a_t}$

The goal is to maximize cumulative reward $\sum^t_{\tau=1}r_\tau$

---

The action-value is the mean reward for action a


$$
q_*(a) \dot= \Bbb E[R_t|A_t=a]
$$


We don't know $q_*(a)$. We denote the estimated value of action a at time step t as $Q_t(a)$. We would like $Q_t(a)$ to be close to $q_*(a)$.

action have the greatest estimated value call greedy action. select greedy action call exploiting.

greedy action selection method as $A_t \dot=\underset a {\arg\max} Q_t(a)$

Greedy action selection always exploits current knowledge to maximize immediate reward.

select one of the nongreedy actions are exploring.

Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.

we can't do exploit and explore at the same time.

A simple alternative is $\epsilon$-greedy methods. which is behave greedily most of the time, but with small probability $\epsilon$ select randomly from among all the actions with equal probability.

---

### Incremental Implementation

The action-value methods we have discussed so far all estimate action values as sample averages of observed rewards. Incremental implementation can computed these averages in a computationally efficient manner, in particular, with constant memory and constant per-time-step computation.

Incremental formulas for updating averages with small, constant computation required to process each new reward. 


$$
Q_n \dot=\frac {R_1+R_2+ \cdots + R_{n-1}} {n-1}
$$


Given $Q_n$ and the $n$th reward, $R_n$, the new average of all n rewards can be computed by


$$
Q_{n+1} = \frac 1 n \overset n{\underset {i=1} \sum}R_i \\
= Q_n + \frac 1 n [R_n - Q_n]
$$


#### Tracking a Nonstationary Problem

We often encounter reinforcement learning problems that are effectively nonstationary. that is, the reward probabilities change over time. In such cases it makes sense to give more weight to recent rewards than to long-past rewards.


$$
Q_{n+1} \dot= Q_n + \alpha[R_n-Q_n]
$$


where the step-size parameter $\alpha \in (0, 1]$ is constant.

**Optimistic Initialization**: Simple and practical idea is initialize Q(a) to high value and update action value by incremental Monte-Carlo evaluation. It will encourages systematic exploration early on.

---

### Upper-Conﬁdence-Bound Action Selection

The idea of this upper conﬁdence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of $a$’s value.

It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. One effective way of doing this is to select actions according to


$$
A_t \dot= \underset a {\arg\max}[Q_t(a)+ c\sqrt{\frac {\ln t} {N_t(a)}}]
$$


$N_t(a)$ denotes the number of times that action $a$ has been selected prior to time $t$.  Each time $a$ is selected the uncertainty is presumably reduced.

---

### Gradient Bandit

Here we consider learning a numerical preference for each action $a$, which we denote $H_t(a)$. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important


$$
Pr\{A_t=a\} \dot= \frac {e^{H_t(a)}} {\sum^k_{b=1}e^{H_t(b)}} \dot=\pi_t(a)
$$


On each step, after selecting action $A_t$ and receiving the reward $R_t$, the action preferences are updated by:


$$
H_{t+1}(A_t) \dot=H_t(A_t) + \alpha(R_t-\overset -R_t)(1-\pi_t(A_t)), and\\
H_{t+1}(a) \dot=H_t(a) - \alpha(R_t-\overset -R_t)\pi_t(a), for\ all\ a \not= A_t
$$


If the reward is higher than the baseline $\overset - R_t$, then the probability of taking $A_t$ in the future is increased, and if the reward is below baseline, then the probability is decreased. The non-selected actions move in the opposite direction. 

---

### Summary

Exploration and Exploitaion

* Exploration ﬁnds more information about the environment
* Exploitation exploits known information to maximize reward
* It is usually important to explore as well as exploit

Several simple ways of balancing exploration and exploitation. 

* The $\epsilon$-greedy methods choose randomly a small fraction of the time,
* UCB methods choose deterministically but achieve exploration by subtly favoring at each step the actions that have so far received fewer samples. 
* Gradient bandit algorithms estimate not action values, but action preferences, and favor the more preferred actions in a graded, probabilistic manner using a soft-max distribution. 
* The simple expedient of initializing estimates optimistically causes even greedy methods to explore signiﬁcantly. 

Reference:

Reinforcement Learning An Introduction

http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/XX.pdf