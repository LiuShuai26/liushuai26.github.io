---
layout: post
title:  "Policy Gradient Algorithms"
date:   2020-12-26 01:20:00 +0800
categories: rl




---

> Policy Gradient Algorithms include VPG, PPO, A3C, DDPG, TD3, SAC.

## VPG

**Policy Optimization.** Methods in this family represent a policy explicitly as $\pi_\theta(a\|s)$. They optimize the parameters $\theta$ either directly by gradient ascent on the performance objective $J(\pi_\theta)$, or indirectly, by maximizing local approximations of $J(\pi_\theta)$.

The performance objective $J(\pi_\theta)$ here is the reward. The reward function is defined as:


$$
J(\theta) = \sum_{s \in S}d^\pi(s)V^\pi(s) = \sum_{s \in S}d^\pi(s)\sum_{a \in A}\pi_\theta(a|s)Q^\pi(s, a)
$$




where $d^\pi(s)$ is the stationary distribution of Markov chain for $\pi_\theta$ (on-policy state distribution under $\pi$). 

Using *gradient ascent*, we can move θ toward the direction suggested by the gradient $\nabla_\theta J(\theta)$ to find the best θ for $\pi_\theta$ that produces the highest return.

VPG Key Equations:

The gradient of $J(\theta)$ is


$$
\nabla_\theta J(\pi_\theta) = \Bbb E_{\pi_\theta}[\nabla_\theta\log{\pi_\theta}(s,a)Q^{\pi_\theta}(s,a)]
$$

$$
\nabla_\theta J(\pi_\theta) =\underset {\tau \sim \pi_\theta} {\Bbb E}[\overset T {\underset {t=0} \sum} \nabla_\theta \log \pi_\theta(a_t|s_t)A^{\pi_\theta}(s_t, a_t)]
$$



This optimization is almost always performed **on-policy**, which means that each update only uses data collected while acting according to the most recent version of the policy. Policy optimization also usually involves learning an approximator $V_\phi(s)$ for the on-policy value function $V^\pi(s)$, which gets used in figuring out how to update the policy.

Policy-based reinforcement learning method Ada:

- better convergence properties, just follow the gradient to find the best parameters
- more effective in high dimensional action spaces
- can learn stochastic policies, We also get rid of the problem of perceptual aliasing. Perceptual aliasing is when we have two states that seem to be (or actually are) the same, but need different actions.

Dis:

- Need a lot of the time, they converge on a local maximum rather than on the global optimum.

  Solutions: Policy Search

In practice, $V^\pi(s_t)$ cannot be computed exactly, so it has to be approximated. This is usually done with a neural network, $V_\phi(s_t)$, which is updated concurrently with the policy (so that the value network always approximates the value function of the most recent policy).

The simplest method for learning , used in most implementations of policy optimization algorithms (including VPG, TRPO, PPO, and A2C), is to minimize a mean-squared-error objective:


$$
\phi_k = \arg\underset \phi \min \underset {s_t, \hat R_t \sim \pi_k} E [(V_\phi(s_t)-\hat R_t)^2]
$$

---

## PPO

PPO is motivated by the same question as TRPO: how can we take the biggest possible improvement step on a policy using the data we currently have, without stepping so far that we accidentally cause performance collapse?

PPO-Clip doesn't have a KL-divergence term in the objective and doesn't have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.

PPO-clip updates policies via (by maximizing the objective)


$$
\theta_{k+1} = \arg \underset \theta \max \underset {s, a \sim \pi_{\theta_k}} E[L(s, a, \theta_k, \theta)]
$$


$\epsilon$ is a (small) hyperparameter which roughly says how far away the new policy is allowed to go from the old.

$L(s, a, \theta_k, \theta)$ is the PPO-Clip objective function:


$$
L(s,a,\theta_k,\theta) = \min(\frac {\pi_\theta(a|s)} {\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a), g(\epsilon, A^{\pi_{\theta_k}}(s,a))\\
where \ g(\epsilon, A) = \begin{cases} (1+\epsilon)A & {A \geq 0} \\ (1-\epsilon)A & A < 0 \end{cases}
$$



Cause we want reuse data collect by old policy(after once gradient ascent, the police collected data became old policy). We use Importance sampling to enable use data from different policy. 

Importance sampling function: $E_{x\sim p}[f(x)] = E_{x\sim p}[\frac {p(x)} {q(x)} f(x)]$. This is where $\frac {\pi_\theta(a\|s)} {\pi_{\theta_k}(a\|s)}A^{\pi_{\theta_k}}(s,a)$ come from.

**Advantage is positive**: Suppose the advantage for that state-action pair is positive, in which case its contribution to the objective reduces to


$$
A \geq 0 \  L(s,a,\theta_k,\theta) = \min(\frac {\pi_\theta(a|s)} {\pi_{\theta_k}(a|s)}, (1+\epsilon))A^{\pi_{\theta_k}}(s,a)
$$


Advantage is positive which means this action a under state s is better than others. So we want increase this action's probability under state s. But the probability incensement can't great than (1+$\epsilon$).

**Advantage is negative**: Suppose the advantage for that state-action pair is negative, in which case its contribution to the objective reduces to


$$
A < 0 \  L(s,a,\theta_k,\theta) = \min(\frac {\pi_\theta(a|s)} {\pi_{\theta_k}(a|s)}, (1-\epsilon))A^{\pi_{\theta_k}}(s,a)
$$



Advantage is negative which means this action a under state s is worse than others. So we want decrease this action's probability under state s. But the probability decrement can't less than (1+$\epsilon$).

SPPO


$$
L(s,a,\theta_k,\theta) = \min(\frac {\pi_\theta(a|s)} {\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a), g(\epsilon, A^{\pi_{\theta_k}}(s,a)) + \alpha \pi_\theta(\widetilde a|s) \log \pi_\theta(\widetilde a|s)\\
where \ g(\epsilon, A) = \begin{cases} (1+\epsilon)A & {A \geq 0} \\ (1-\epsilon)A & A < 0 \end{cases}
$$

---

## A3C

Deep neural networks provide rich representations that can enable reinforcement learning (RL) algorithms to perform effectively.

the combination of simple online RL algorithms with DNN was fundamentally unstable. 

Why?

the sequence of observed data encountered by an online RL agent is non-stationary, and online RL updates are strongly correlated. 

Solution: Replay buffer

the data can be batched or randomly sampled from different time-steps.

reduce non-stationarity and decorrelates updates. 

Drawbacks:

Limits the methods to off-policy RL algorithms.

It uses more memory and computation per real interaction 

This paper:

Idea: asynchronously execute multiple agents in parallel, on multiple instances of the environment. 

enables on-policy and off-policy RL algorithms.

run on multi-core CPU. using far less resource.



Asynchronous RL Framework

multi-threaded asynchronous variants of one-step Sarsa, one-step Q-learning, n-step Q-learning, and advantage actor-critic.

two main ideas:

First, asynchronous actor-learners, use multiple CPU threads on a single machine.

Second, we make the observation that multiple actors-learners running in parallel are likely to be exploring different parts of the environment.



Asynchronous advantage actor-critic: 

We typically use a convolutional neural network that has one softmax output for the policy $\pi(a_t\|s_t;\theta)$ and one linear output for the value function $V(s_t;\theta_v)$, with all non-output layers shared.

We also found that adding the entropy of the policy $\pi$ to the objective function improved exploration by discouraging premature convergence to suboptimal deterministic policies.

asynchronous advantage actor-critic (A3C), maintains a policy $\pi(a_t\|s_t;\theta)$ and an estimate of the value function $V(s_t; \theta_v)$. uses the same mix of n-step returns to update both the policy and the value-function. The policy and the value function are updated after every $t_\max$ actions or when a terminal state is reached. The update performed by the algorithm can be seen as $\nabla_{\theta'} \log\pi(a_t \| s_t; \theta')A(s_t, a_t; \theta, \theta_v)$ 

where $A(s_t, a_t;\theta, \theta_v)$ is an estimate of the advantage function given by $\sum_{i=0}^{k-1} \gamma^i r_{t+i} + \gamma^kV(s_{t+k};\theta_v)−V(s_t;\theta_v)$



A*3

**Asynchronous**: Unlike DQN, where a single agent represented by a single neural network interacts with a single environment, A3C utilizes multiple incarnations of the above in order to learn more efficiently. In A3C there is a global network, and multiple worker agents which each have their own set of network parameters. Each of these agents interacts with it’s own copy of  the environment at the same time as the other agents are interacting with their environments. The reason this works better than having a single agent (beyond the speedup of getting more work done), is that the experience of each agent is independent of the experience of the others. In this way the overall experience available for training becomes more diverse.

**Actor-Critic**:  Actor-Critic combines the benefits of Q-learning and Policy Gradient approaches. In the case of A3C, our network will estimate both a value function **V(s)** (how good a certain state is to be in) and a policy **π(s)**  (a set of action probability outputs). These will each be separate fully-connected layers sitting at the top of the network. **Critically, the agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods.**

**Advantage**: $A = Q(s, a) - V(s)$. Since we won't be determining the Q values directly in A3C, we can use the discounted returns (R) as an estimate of Q(s,a) to allow us to generate an estimate of the advantage: $A = R - V(s), R = \gamma(r)$

---

## DDPG, TD3

### The Optimal Q-Function and the Optimal Action

There is an important connection between the optimal action-value function $Q^\*(s, a)$ and the action selected by the optimal policy. By definition, $Q^\*(s, a)$ gives the expected return for starting in state $s$, taking (arbitrary) action $a$, and then acting according to the optimal policy forever after.

The optimal policy in $s$ will select whichever action maximizes the expected return from starting in $s$. As a result, if we have $Q^*$, we can directly obtain the optimal action, $a^\*(s)$, via

$a^*(s) = \arg \max_aQ^\*(s, a)$

Note: there may be multiple actions which maximize $Q^\*(s, a)$,  in which case, all of them are optimal, and the optimal policy may randomly select any of them. But there is always an optimal policy which deterministically selects an action.

### DDPG

Deep Deterministic Policy Gradient (DDPG) is an algorithm which concurrently learns a Q-function and a policy. **It uses off-policy data and the Bellman equation to learn the Q-function, and uses the Q-function to learn the policy.**

Because the action space is continuous, the function $Q^*(s, a)$ is presumed to be differentiable with respect to the action argument. This allows us to set up an efficient, gradient-based learning rule for a policy $\mu(s)$ which exploits that fact. Then, instead of running an expensive optimization subroutine each time we wish to compute $\max_aQ(s, a)$, we can approximate it with $\max_aQ(s, a) \approx Q(s, \mu(s))$.

The Q-Learning Side of DDPG

Loss function: **mean-squared Bellman error (MSBE)**

$L(\phi, D) = E_{(s, a, r, s', d)\sim D}\left[\left(Q_\phi(s, a)-(r+\gamma(1-d)\max_{a'}Q_\phi(s', a'))\right)^2\right]$

d mean done, if d = 1 (True), s' is a terminal state.

Q-learning algorithms for function approximators, such as DQN (and all its variants) and DDPG, are largely based on minimizing this MSBE loss function. 

It uses off-policy data and the Bellman equation to learn the Q-function, 

Again, we can't compute the $\max_{a'}Q_\phi(s', a')$ in continuous action spaces. So we use a **target policy network** to compute an action.

The Policy Learning Side of DDPG

$L(\phi, D) = E_{(s, a, r, s', d)\sim D}\left[\left(Q_\phi(s, a)-(r+\gamma(1-d)Q_{\phi_{targ}}(s', \mu_{\theta_{targ}}(s')))\right)^2\right]$

where $\mu_{\theta_{targ}}$ is the target policy.

Policy learning in DDPG is fairly simple. We want to learn a deterministic policy $\mu_\theta(s)$ which gives the action that maximizes $Q_\phi(s, a)$. Because the action space is continuous, and we assume the Q-function is differentiable with respect to action, we can just perform gradient ascent (with respect to policy parameters only) to solve $\max_\theta E_{s\sim D}[Q_\phi(s, \mu_\theta(s))].$

Note that the Q-function parameters are treated as constants here.

Tricks:

- **Target Networks.** Q-learning algorithms make use of **target networks**.
- **Replay Buffers.** All standard algorithms for training a deep neural network to approximate $Q^*(s, a)$ make use of an experience replay buffer. 
- **OU noise.** To make DDPG policies explore better, we add noise to their actions at training time. The authors of the original DDPG paper recommended time-correlated OU noise, but more recent results suggest that uncorrelated, mean-zero Gaussian noise works perfectly well. 
- For exploration at the start of training. For a fixed number of steps at the beginning (set with the `start_steps` keyword argument), the agent takes actions which are sampled from a uniform random distribution over valid actions. After that, it returns to normal DDPG exploration.



### TD3

**Trick One: Clipped Double-Q Learning.** TD3 learns *two* Q-functions instead of one (hence “twin”), and uses the smaller of the two Q-values to form the targets in the Bellman error loss functions.

**Trick Two: “Delayed” Policy Updates.** TD3 updates the policy (and target networks) less frequently than the Q-function. The paper recommends one policy update for every two Q-function updates.

**Trick Three: Target Policy Smoothing.** TD3 adds noise to the target action, to make it harder for the policy to exploit Q-function errors by smoothing out Q along changes in action.

Together, these three tricks result in substantially improved performance over baseline DDPG.

TD3 trains a deterministic policy in an off-policy way. To make TD3 policies explore better, we add noise to their actions at training time, typically uncorrelated mean-zero Gaussian noise.

For exploration at the start of training. For a fixed number of steps at the beginning (set with the `start_steps` keyword argument), the agent takes actions which are sampled from a uniform random distribution over valid actions. After that, it returns to normal TD3 exploration.

---

### SAC

A central feature of SAC is **entropy regularization.** The policy is trained to maximize a trade-off between expected return and entropy, a measure of randomness in the policy.

**Entropy-Regularized Reinforcement Learning**

Let $x$ be a random variable with probability mass or density function $P$. The entropy $H$ of $x$ is computed from its distribution $P$ according to


$$
H(P) = \underset {x \sim P} E[-\log P(x)].
$$


The RL problem now:


$$
\pi^* = \arg \underset \pi \max \underset {\tau \sim \pi} E\left[\overset \infty {\underset {t=0} \sum}\gamma^t\left(R(s_t, a_t, s_{t+1})+\alpha H(\pi(\cdot|s_t))\right) \right]
$$


where $\alpha > 0$ is the trade-off coefficient. 

$V^\pi$ is changed to include the entropy bonuses from every timestep:


$$
V^\pi(s) = \underset {\tau \sim \pi} E \left[\overset \infty {\underset {t=0} \sum}\gamma^t\left(R(s_t, a_t, s_{t+1})+\alpha H(\pi(\cdot|s_t)) \right)|s_0=s \right]
$$


$Q^\pi$ is changed to include the entropy bonuses from every timestep *except the first*:


$$
Q^\pi(s, a) = \underset {\tau \sim \pi} E \left[\overset \infty {\underset {t=0} \sum}\gamma^t R(s_t, a_t, s_{t+1})+\alpha \overset \infty {\underset {t=1} \sum}\gamma^t H(\pi(\cdot|s_t)) |s_0=s,a_0=a \right]
$$


With these definitons, $V^\pi$ and $Q^\pi$ are connected by:


$$
V^\pi(s) = \underset {a\sim\pi}E[Q^\pi(s,a)] + \alpha H(\pi(\cdot|s))
$$


and the Bellman equation for $Q^\pi$ is


$$
\begin{align}
Q^\pi(s,a) &= \underset {s'\sim P}E[R(s,a,s')+\gamma(Q^\pi(s',a')+\alpha H(\pi(\cdot|s')))]
\\&= \underset {s'\sim P}E[R(s,a,s')+\gamma V^\pi(s')].
\end{align}
$$



The modern version that omits the extra value function:

SAC concurrently learns a policy $\pi_\theta$ and two Q-functions $Q_{\phi_1}, Q_{\phi_2}$. There are two variants of SAC that are currently standard: one that uses a fixed entropy regularization coefficient $\alpha$, and another that enforces an entropy constraint by varying $\alpha$ over the course of training. For simplicity, Spinning Up makes use of the version with a fixed entropy regularization coefficient, but the entropy-constrained variant is generally preferred by practitioners.

**Learning Q.** The Q-functions are learned in a similar way to TD3, but with a few key differences.

First, what’s similar?

1. Like in TD3, both Q-functions are learned with MSBE minimization, by regressing to a single shared target.
2. Like in TD3, the shared target is computed using target Q-networks, and the target Q-networks are obtained by polyak averaging the Q-network parameters over the course of training.
3. Like in TD3, the shared target makes use of the **clipped double-Q** trick.

What’s different?

1. Unlike in TD3, the target also includes a term that comes from SAC’s use of entropy regularization.
2. Unlike in TD3, the next-state actions used in the target come from the **current policy** instead of a target policy.
3. Unlike in TD3, there is no explicit target policy smoothing. TD3 trains a deterministic policy, and so it accomplishes smoothing by adding random noise to the next-state actions. SAC trains a stochastic policy, and so the noise from that stochasticity is sufficient to get a similar effect.

Recursive Bellman equation:


$$
Q^\pi(s,a) = \underset {s'\sim P, a'\sim\pi} {\mathbb E}[R(s,a,s')+\gamma(Q^\pi(s',a')+\alpha H(\pi(\cdot|s')))]\\
=  \underset {s'\sim P, a'\sim\pi} {\mathbb E}[R(s,a,s')+\gamma(Q^\pi(s',a')-\alpha \log\pi(a'|s'))]
$$


The R H S is an expectation over next states (which come from the replay buffer) and next actions $\tilde a'$ (which come from the current policy). Since it's an expectation, we can approximate it with samples:


$$
Q^\pi(s,a) \approx r + \gamma(Q^\pi(s',\tilde a')-\alpha\log\pi(\tilde a'|s')),\tilde a'\sim \pi(\cdot|s')
$$


the loss functions for the Q-networks in SAC:


$$
L(\phi_i, \mathcal D) = \underset {(s,a,r,s',d)\sim \mathcal D} E[(Q_{\phi_i}(s,a)-y(r,s',d))^2],
$$


target is:


$$
y(r,s',d) = r + \gamma(1-d)(\underset {j=1,2} \min Q_{\phi_{targ,j}}(s',\tilde a')-\alpha\log\pi_\theta(\tilde a'|s')), \tilde a'\sim\pi_\theta(\cdot|s').
$$


**Learning the Policy.** The policy should, in each state, act to maximize the expected future return plus expected future entropy. That is, it should maximize $V^\pi(s)$, which we expand out into


$$
V^\pi(s) = \underset {a\sim \pi} E[Q^\pi(s,a)]+\alpha H(\pi(\cdot|s))\\
= \underset {a\sim \pi} E[Q^\pi(s,a)-\alpha\log\pi(a|s)].
$$


The way we optimize the policy makes use of the **reparameterization trick**, in which a sample from $\pi_\theta(\cdot\|s)$ is drawn by computing a deterministic function of state, policy parameters, and independent noise:


$$
\tilde a_\theta(s,\xi) = \tanh(\mu_\theta(s)+\sigma_\theta(s) \odot \xi), \xi\sim\mathcal N(0, I)
$$


policy loss:


$$
\underset {j=1,2} \min Q_{\phi_j}(s,\tilde a_\theta(s,\xi))-\alpha\log\pi_\theta(\tilde a_\theta(s,\xi)|s)
$$



At test time, to see how well the policy exploits what it has learned, we remove stochasticity and use the mean action instead of a sample from the distribution. This tends to improve performance over the original stochastic policy.

---





























